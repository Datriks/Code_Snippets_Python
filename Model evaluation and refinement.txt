## Model Evaluation and Refinement
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
### Import data
path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/automobileEDA.csv'
df = pd.read_csv(path)
df.head()
#### Save file locally
df.to_csv('module_5_auto.csv')
Lets extract just the numeric variables
df = df._get_numeric_data()
df.head()
#### Libraries for ploting
# conda install -c conda-forge ipywidgets
from ipywidgets import interact, interactive, fixed, interact_manual
#### Create function with plotting characteristics
def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):
    width = 15
    height = 6
    plt.figure(figsize=(width, height))

    ax1 = sns.distplot(RedFunction, hist = False, color = 'r', label = RedName)
    ax2 = sns.distplot(BlueFunction, hist = False, color ='b', label = BlueName, ax = ax1)

    plt.title(Title)
    plt.xlabel('Price in dollars')
    plt.ylabel('Proportion of cars')

    plt.show()
    plt.close()
def PollyPlot(xtrain, xtest, y_train, y_test, lr, poly_transform):
    width = 12
    height = 10
    plt.figure(figsize=(width, height))

    # training data
    # testing data
    # lr: linear regression object
    # poly_transform : polynimial transformation object

    xmax = max([xtrain.values.max(), xtest.values.max()])
    xmin = min([xtrain.values.min(), xtest.values.min()])

    x = np.arange(xmin, xmax, 0.1)

    plt.plot(xtrain, y_train, 'ro', label = 'Training Data')
    plt.plot(xtest, y_test, 'go', label = 'Test Data')
    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label = 'Predicted Function')
    plt.ylim(-10000, 60000)
    plt.ylabel('Prce')
    plt.legend()
#### Split the data into train and test
Place the target data into a specific data frame called y_data
y_data = df['price']
Drop price data in dtaframe x_data
x_data = df.drop('price', axis = 1)
Randomly split the data frame into training and testing data using the function train_test_split
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)

# here we fing the proportion of data that is split into testing and training set.
print("number of test samples: ", x_test.shape[0])
print("number of training samples: ", x_train.shape[0])
## Practice
x_train1, x_test1, y_train1, y_test1 = train_test_split(x_data, y_data, test_size=0.4, random_state=0)

print("number of test samples: ", x_test1.shape[0])
print("number of training samples: ", x_train1.shape[0])
#### Import the LinearRegression function from module linear_model
from sklearn.linear_model import LinearRegression
#### Create linear Regression object
lre = LinearRegression()
We fit the model using the feature horsepower
model_1 = lre.fit(x_train[['horsepower']], y_train)
Calculate R^2 on test data
R_score_test_1 = lre.score(x_test[['horsepower']], y_test)
R_score_test_1
R_score_train_1 = lre.score(x_train[['horsepower']], y_train)
R_score_train_1
We can see that the R^" of test data is smaller than R^2 train data
#### Find the values of R^2 of tran and test data for the 40% split data
model_2 = lre.fit(x_train1[['horsepower']], y_train1)
R_score_test_11 = lre.score(x_test1[['horsepower']], y_test1)
R_score_test_11
R_score_train_11 = lre.score(x_train1[['horsepower']], y_train1)
R_score_train_11
### Cross Validation Score
Import model_selection from the module cross_val_score
from sklearn.model_selection import cross_val_score
Input the object, feature ("horsepower), the arget data(y_data). The parameter cv determines the number of folds in this case 4.
Rcross_1 = cross_val_score(lre, x_data[['horsepower']], y_data, cv =4)
The default scoring of R^2. Each element in the array has the average R^2 value fot the fold
Rcross_1
Calculate the average and standard deviation of our estimate
print("The mean of the folds is: ", Rcross_1.mean(), "and the standard deviation is :", Rcross_1.std())
Use negative squared error as a score by setting the parameter scorring metric to "neg_mean_squared_error
-1 * cross_val_score(lre, x_data[['horsepower']], y_data, cv = 4, scoring = 'neg_mean_squared_error')
#### Practice : Calculate the average R^2 using two folds and find the average R^2 for the secong fold utilising the horsepower feature
Rc = cross_val_score(lre, x_data[['horsepower']], y_data, cv = 2)
Rc
print('The mean of folds is:', Rc.mean(), 'and the standard deviation is:', Rc.std())
We can predict the output using the function cross_val_predict
from sklearn.model_selection import cross_val_predict
yhat = cross_val_predict(lre, x_data[['horsepower']], y_data, cv = 4)
yhat[0:10]
## Underfitting, Overfitting and Model Selection
The test data or the out of sample data is the best measure how well your model performsin the real world. One reason is the overfitting.  
The differences appear more often in multilinear regression and polynomial regression so we are going to test this in this context.  
Create multilinear object first.
lr = LinearRegression()
model = lr.fit(x_train[['horsepower','curb-weight', 'engine-size', 'highway-mpg']], y_train)
Prediction using training data:
yhat_train = lr.predict(x_train[['horsepower','curb-weight', 'engine-size', 'highway-mpg']])
yhat_train[0:5]
Predict using test data:
yhat_test = lr.predict(x_test[['horsepower','curb-weight', 'engine-size', 'highway-mpg']])
yhat_test[0:5]
#### Perform Model Evaluation
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
Examine the distribution of the predicted values of the training data
Declare the variables:
Title = 'Distributin Plot of Predicted Value using Training Data vs Testing Data Distribution'
DistributionPlot(y_train, yhat_train, 'Actual values (Train)', 'Predicted values (Test)', Title)
The model seems to be doing well in learning from the training dataset.  
Let's see what hapens when the model encounters new data from the testing dataset.
Title = 'Distribution Plot of Predicted Value using Test data vs Data distribution of Test data'
DistributionPlot(y_test, yhat_test, 'Actual values (test)', 'Predicted values (test)', Title)
The distribution of test data is much better at fitting the data.  
This is evident in fig 2. in the range 5000 to 15000.  
Lts's see if polynomial regression also exibits a drop in the prediction accuracy when analysisng the data set.
from sklearn.preprocessing import PolynomialFeatures